# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VHqxMbqtLP81PqviqUV2O7o8yg4DQB63
"""

!pip install gradio
!pip install transformers
!pip install spacy

!pip install requests
!pip install langchain_groq

import gradio as gr
import matplotlib.pyplot as plt
import pandas as pd
import re
import spacy
from transformers import pipeline
from langchain_groq import ChatGroq

# Initialize SpaCy model for NLP tasks
nlp = spacy.load("en_core_web_sm")
reason_for_call = None
# Initialize sentiment analysis pipeline
sentiment_pipeline = pipeline("sentiment-analysis")

# Initialize Groq model
api_key = "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"
llm = ChatGroq(temperature=0.1, groq_api_key=api_key, model_name="mixtral-8x7b-32768")

def clean_text(text):
    """
    Clean the transcript text by removing speaker labels, newlines, and non-alphabetic characters.
    """
    text = re.sub(r'Sales Rep.*?:', '', text)
    text = re.sub(r'Customer.*?:', '', text)
    text = re.sub(r'\n', ' ', text)
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    return text.strip()

def analyze_transcript(review=None, conversation=None):
    """
    Analyze the transcript to determine sentence-level sentiments, overall sentiment,
    reason for the call, and generate a sentiment score plot.
    """
    global reason_for_call
    # Determine which input to use
    transcript = review or conversation
    if not transcript:
        return {"error": "Either review or conversation must be provided."}

    # Clean and process transcript
    cleaned_transcript = clean_text(transcript)
    doc = nlp(cleaned_transcript)

    # Extract sentences
    sentences = [sent.text for sent in doc.sents]

    # Run sentiment analysis on each sentence
    sentiment_results = sentiment_pipeline(sentences)

    # Create DataFrame for results
    df = pd.DataFrame(sentiment_results)
    df['Sentence'] = sentences
    df = df.rename(columns={'label': 'Sentiment', 'score': 'Score'})

    # Calculate overall sentiment with weighted average score approach
    sentiment_scores = {'POSITIVE': 1, 'NEGATIVE': -1, 'NEUTRAL': 0}
    df['SentimentScore'] = df['Sentiment'].map(sentiment_scores)
    weighted_score = df['SentimentScore'].mean()

    # Determine overall sentiment based on weighted score
    if weighted_score > 0.2:
        overall_sentiment = 'POSITIVE'
    elif weighted_score < -0.2:
        overall_sentiment = 'NEGATIVE'
    else:
        overall_sentiment = 'NEUTRAL'

    # Determine reason for the call using Groq model
    prompt = f"""
    Analyze the following text and provide the reason for the call.

    Text:
    {cleaned_transcript}
    """
    try:
        response = llm.invoke(prompt)
        reason_for_call = response.content.strip()
    except Exception as e:
        reason_for_call = f"Error determining reason for call: {str(e)}"

    # Generate sentiment score plot
    plt.figure(figsize=(12, 6))
    color_map = {'POSITIVE': 'green', 'NEGATIVE': 'red', 'NEUTRAL': 'gray'}
    colors = df['Sentiment'].map(color_map)
    df['Score'].plot(kind='bar', color=colors)
    plt.xticks(range(len(sentences)), sentences, rotation=90, fontsize=8)
    plt.xlabel("Sentence")
    plt.ylabel("Sentiment Score")
    plt.title("Sentiment Analysis of Transcript")
    plt.tight_layout()

    # Save plot to a file
    plot_filename = "sentiment_analysis_plot.png"
    plt.savefig(plot_filename)
    plt.close()

    # Compile analysis summary
    analysis_summary = {
        "Overall Sentiment": overall_sentiment,
        "Reason for Call": reason_for_call,
    }

    return df.to_markdown(), analysis_summary, plot_filename

# Gradio Interface
iface = gr.Interface(
    fn=analyze_transcript,
    inputs=[gr.Textbox(label="Review Text", placeholder="Enter review here..."),
            gr.Textbox(label="Conversation Transcript", placeholder="Enter conversation here...")],
    outputs=["markdown", "json", "image"],
    title="Sentiment Analysis for Sales Transcript",
    description="Analyze the sentiment of a sales transcript, extract entities, and visualize sentiment scores.",
    examples=[
        ["Sales Rep: Hello, how are you today?\nCustomer: I'm good, thanks! Just checking on product availability.", ""],
        ["", "Sales Rep: Thank you for calling. How can I assist?\nCustomer: Iâ€™m facing issues with my billing."],
        ["", "Sales Rep: Can I help you with anything else today?\nCustomer: No, just wanted to provide some feedback!"]
    ]
)

# Launch the interface
iface.launch()

print(reason_for_call)

from langchain.chains import RetrievalQA
from langchain.memory import ConversationBufferMemory

!pip install chromadb
!pip install sentence_transformers

!pip install langchain-community

from langchain.vectorstores import Chroma
from langchain.embeddings import SentenceTransformerEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import TextLoader

reason_for_call

# Load documents from a file
loader = TextLoader('//content/instuctions_1.txt')  # Replace with your file path
documents = loader.load()

# Split documents into chunks
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)

# Create embeddings using SentenceTransformer
embeddings = SentenceTransformerEmbeddings(model_name="all-mpnet-base-v2")

# Initialize Chroma vector store
vector_search = Chroma.from_documents(texts, embeddings, persist_directory="./chroma_db")

from langchain.prompts import PromptTemplate


# Define the custom prompt template
custom_prompt_template = """
You are a helpful assistant. Based on the context provided and the reason for the call, provide clear instructions to resolve the customer's issue.


Context: {context}

Helpful answer:
"""

# Initialize the PromptTemplate with input variables
template_prompt = PromptTemplate(
    template=custom_prompt_template,
    input_variables=[ "context"]  # Add 'context' here
)

# Initialize the RetrievalQA chain with the custom prompt
qa = RetrievalQA.from_chain_type(
    llm=llm,  # Ensure 'llm' is defined elsewhere in your code
    chain_type="stuff",
    retriever=vector_search.as_retriever(search_type="similarity", search_kwargs={"k": 3}),
    chain_type_kwargs={"prompt": template_prompt},  # Removed memory here
)

# ... previous code ...

# Function to generate the response
def generate_response():
    try:
        # Call the chain with context and reason for call
        # Assuming 'reason_for_call' is a global variable
        global reason_for_call
        # This line is changed!
        answer = qa({"query": "give me the instructions", "context": "Provide clear instructions to resolve the customer's issue based on the reason for call. Reason for call: " + reason_for_call})
        return answer
    except Exception as e:
        return f"Error processing the request: {e}"

import gradio as gr



# Gradio interface
with gr.Blocks() as demo:
    gr.Markdown("# Automated instructions Generator")

    # Button to trigger response generation
    generate_button = gr.Button("Generate Instuctions for Reason")

    # Textbox to display the generated response
    output_text = gr.Textbox(label="Instructions")

    # Define the action when the button is clicked
    generate_button.click(
        fn=generate_response,
        inputs=[],
        outputs=output_text,
    )

if __name__ == "__main__":
    demo.launch()